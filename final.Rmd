---
title: "Final Project"
author: "Ziyi Yang & Xiaoyi Wu"
date: "12/2/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    fig_width: 10
    fig_height: 9
    theme: united
    highlight: tango
---

## I. Introduction


## II. Setup and Data Loading

### 1. Setup

In this section, we loaded necessary libraries, created plot theme options and map theme options, and identified functions of quintile breaks, and average nearest neighbor distance for further analysis.

```{r Load Libraries, echo=TRUE, message=FALSE, warning=FALSE, results=FALSE}
# 1. load Libraries
library(sf)
library(tidyverse)
# install.packages('mapview')
library(mapview)
library(spdep)
library(caret)
library(ckanr) # for opening data APIs built on CKAN technology
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(stargazer) # for creating table
library(broom)
library(tufte)
library(rmarkdown)
library(kableExtra)
library(tidycensus)
library(RSocrata)
library(viridis)
library(spatstat) # make kernel density map
library(raster)
library(knitr)
library(rgdal)
# 2. Identify functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

### 2. Data Preparation

**1. Base Map Datasets**

* [**Cincinnati Boundary**](https://data-cagisportal.opendata.arcgis.com/datasets/cincinnati-city-boundary/explore): A base and outline of the City of Cincinnati.https://opendata.arcgis.com/datasets/ed78f4754b044ac5815d0a9efe9bb336_1.geojson 

* [**Cincinnati Neighborhoods**](https://data-cagisportal.opendata.arcgis.com/datasets/cincinnati-community-council-boundaries/explore): A base and outline of the neighborhoods of Cincinnati. 
https://opendata.arcgis.com/datasets/fff393f0112544b397838f9cf4d7765a_1.geojson
* [**Police Districts**](https://data-cagisportal.opendata.arcgis.com/datasets/police-districts/explore): Geojson file contains police district ploygon information. https://opendata.arcgis.com/datasets/9bc1afaff72e4f44a6d19280c159c951_4.geojson

**2. Opioid  Overdose Data**: This analysis focuses on opioid overdose cases that also suffers from selection bias, such as racial bias when it comes to prediction. 

* [**Opioid Overdose**](https://github.com/sydng/Cincinatti_Overdose_Data): This repository hosts a now deprecated version of EMS data for opioid overdose dispatches for Cincinnati, OH with xy coordinates. Data for July 2015 through November 2017 is included.

**3. Risk Factors Data**: This analysis selects five point level features that may intrigue shooting cases to build the model. See each dataset below:

* [**Building Demolitions**](https://opendataphilly.org/dataset/building-demolitions): A point feature class of demolitions occurred in 2018, performed by private owners/contractors and by the Department of Licenses and Inspections due to dangerous building conditions.

* [**Vacant Land**](https://data-phl.opendata.arcgis.com/datasets/vacant-indicators-points): A point feature class of the location of properties across Philadelphia that are likely to be a vacant land based on an assessment of City of Philadelphia administrative datasets.

* [**Tobacco Retailer**](https://data-phl.opendata.arcgis.com/datasets/affordable-housing/data): A point feature class of the location of tobacco retailers that applied for a tobacco permit through an online application system in 2018.

* [**Tobacco Youth Sales Violations**](https://www.opendataphilly.org/dataset/tobacco-youth-sales-violations): This dataset contains violations for tobacco sale to minors in 2018. 

* [**Affordable Housing**](https://data-phl.opendata.arcgis.com/datasets/affordable-housing/data): A point feature class of the affordable housing units built in or before 2018.

* [**Census Data**](): demographic variables from the ACS 2018 for census tracts in City of Cincinnati. This analysis mainly focuses on median household income, race context, poverty level, and umemployment rate.


```{r dataset, message=FALSE, warning=FALSE, results = FALSE}
# polygon
cinPD <- 
  st_read("https://opendata.arcgis.com/datasets/9bc1afaff72e4f44a6d19280c159c951_4.geojson") %>%
  st_transform('ESRI:102258')

cinPolice <- st_read("https://opendata.arcgis.com/datasets/f19b190bf6df43dea22433ccc0103911_3.geojson") %>%
  st_transform('ESRI:102258')

cinBoundary <- 
  st_read("https://opendata.arcgis.com/datasets/ed78f4754b044ac5815d0a9efe9bb336_1.geojson") %>%
  st_transform('ESRI:102258') 

#2016 Opioid Overdose Data
opioid <- 
  st_read("C:/Users/y4ngz/Desktop/Cinci_Overdoses.geojson") %>% 
  filter(grepl(2016,CREATE_TIM)) %>%
  st_transform('ESRI:102258') 

opioid17 <- 
  st_read("C:/Users/y4ngz/Desktop/Cinci_Overdoses.geojson") %>% 
  filter(grepl(2017,CREATE_TIM)) %>%
  st_transform('ESRI:102258') 

grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = cinBoundary) +
  geom_sf(data = opioid, colour="red", size=0.1, show.legend = "point") +
  labs(title= "weapon violation, Chicago - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = cinBoundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(opioid)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of weapon violation") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))

# Creating a fishnet grid
cin_fishnet <- 
  st_make_grid(cinBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[cinBoundary] %>% 
  st_sf() %>%
  mutate(uniqueID = rownames(.))

cinNeigh <-
  st_read("https://opendata.arcgis.com/datasets/fff393f0112544b397838f9cf4d7765a_1.geojson") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet)) 

# health center
health_center <-
  st_read("https://data.cincinnati-oh.gov/resource/v8yh-wpss.csv")%>%
  dplyr::select(Y = latitude, X = longitude) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258')

hospital <- 
  st_read("https://opendata.arcgis.com/datasets/0baa0cdf409e46e1800a6f08081bd9f7_1.geojson")%>%
  st_transform('ESRI:102258')

# import risk factors
request_311 <-
  st_read("C:/Users/y4ngz/Desktop/A/MUSA 508/Final/Cincinnati_311__Non-Emergency__Service_Requests.csv") %>% 
  filter(grepl(2016,REQUESTED_DATE))

dead_animals <- request_311%>%
  filter(grepl("DAPUB1",SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "dead_animals")

abandon_cars <- request_311%>%
  filter(grepl("ABAN-VPR", SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "abandon_cars")  

pothole <- request_311%>%
  filter(grepl("PTHOLE",SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "pothole")

trash <- request_311%>%
  filter(grepl("RF-COLLT",SERVICE_CODE) |grepl("TRASH-I",SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "trash")

graffiti <- request_311%>%
  filter(grepl("GRFITI",SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "graffiti")

street_cleaning <- request_311%>%
  filter(grepl("SCLEN1",SERVICE_CODE))%>%
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102258') %>%
  st_transform(st_crs(cin_fishnet))%>%
  mutate(Legend = "street_cleaning")

# Census data
# View(load_variables(2018,'acs5',cache = TRUE))
tracts16 <- 
  get_acs(geography = "tract", variables = c("B00001_001E","B02001_002E","B19013_001E","B25002_001E","B06012_002E","B27011_008E"), 
          year=2016, state="OH", county="Hamilton", geometry=T, output="wide") %>%
  st_transform('ESRI:102258') %>%
  rename(TotalPop = B00001_001E, 
         Whites = B02001_002E,
         MedHHInc = B19013_001E,
         TotalUnit = B25002_001E,
         TotalPoverty = B06012_002E,
         TotalUnemployment = 	B27011_008E) %>%
dplyr::select(-NAME, -starts_with("B")) %>% #-starts_with("B") awesome!
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop * 100,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop *100, 0),
         pctUnemploy = ifelse(TotalPop > 0, TotalUnemployment / TotalPop *100, 0)
         ) %>%
  dplyr::select(-Whites, -TotalPoverty ,-TotalUnemployment,-GEOID) %>%
  st_transform(st_crs(cin_fishnet)) 

tracts16.MedHHInc <- tracts16 %>%
  dplyr::select(MedHHInc) %>%
  rename(Legend = MedHHInc)
tracts16.pctWhite <- tracts16 %>%
  dplyr::select(pctWhite)%>%
  rename(Legend = pctWhite)
tracts16.pctPoverty <- tracts16 %>%
  dplyr::select(pctPoverty)%>%
  rename(Legend = pctPoverty)
tracts16.pctUnemploy <- tracts16 %>%
  dplyr::select(pctUnemploy)%>%
  rename(Legend = pctUnemploy)
```

## III. Visualizaion and Analysis

***METHOD***This analysis uses open sourced data from the City of Philadelphia’s Open Data Portal and the US Census Bureau’s American Community Survey. From these sources, I wrangle data and design features for risk factors involving income, education, age, and demographics. Using a fishnet grid, I break the city into square cells to design a model which predicts the risk of shooting cases occurrence in each cell based on its risk factor data.
### (I) Shooting Crime Exploratory Analysis
#### 1. A map of shooting crime in 2018, Philadelphia
Figure 1.1 and 1.2 below show the distribution of Philadelphia observed shooting crimes in 2018. Obviously, Oak Lane, Upper North, North, West, and South West are suffering more from shooting crimes compared to other neighborhoods. Considering those neighborhoods are also known for poverty and less-development, otential model might identify areas in a neighborhood where serious shooting crime are more likely to occur during a particular period because its social environment such as less-developed economy, gather of racial minority, and etc where the selection bias might occur.
```{r opioid, echo=TRUE, message=FALSE, warning=FALSE}
# 1. A map of opioid overdose cases in 2016, Cincinnati
grid.arrange(ncol=2,
             ggplot() + 
               geom_sf(data = cinBoundary) +
               geom_sf(data = opioid, colour="darkred", size=0.1, show.legend = "point") +
               labs(title= "Opioid Overdose Cases in 2016",
                    subtitle = 'Cincinnnati, OH\n',
                    caption = 'Figure 1.1') +
               mapTheme() +
               plotTheme(),
             
             ggplot() + 
               geom_sf(data = cinBoundary, fill = "#E5E5E5") +
               stat_density2d(data = data.frame(st_coordinates(opioid)), 
                              aes(X, Y, fill = ..level.., alpha = ..level..),
                              size = 0.01, bins = 40, geom = 'polygon') +
               scale_fill_viridis_c(option = "plasma") +
               scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
               labs(title = "Density of opioid overdose Cases in 2016",
                    subtitle = 'Cincinnati, OH\n',
                    caption = 'Figure 1.2') +
               mapTheme() + 
               theme(legend.position = "none") +
               plotTheme())
```
#### 2. A map of shooting crime joined to the fishnet
Figure 2 presents the shooting cases data in a fishnet which is the basic scale this analysis mainly works at. Similarly, shooting crime are more common in North Philly and West Philly.
```{r opioidnet, echo=TRUE, message=FALSE, warning=FALSE}
# Aggregate points to the fishnet
opioid_net <- 
  dplyr::select(opioid) %>% 
  mutate(countopioid = 1) %>% 
  aggregate(., cin_fishnet, sum) %>%
  mutate(countopioid = replace_na(countopioid, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(cin_fishnet) / 24), 
                       size=nrow(cin_fishnet), replace = TRUE))
ggplot() +
  geom_sf(data = opioid_net, aes(fill = countopioid), color = NA) +
  scale_fill_viridis() +
  labs(title = "Observed Opioid overdose Joined to Fishnet, 2016",
       subtitle = 'Cincinnati, OH\n',
       caption = 'Figure 2') +
  mapTheme() +
  plotTheme()
```
### (II) Risk Factor Exploratory Analysis
To build a model which can capture more characteristics of shooting occurrence pattern, this analysis selects risk factors that are correlated with shooting more or less based on previous studies. These risk factors are building demolitions occurring within the City of Philadelphia, legal tobacco retailer stations, affordable housing level, tobacco sale violations to minors, vacant land ratio, and other demographic factors (household income, poverty level, unemployment rate, and racial group) fetched from census data. 
#### 3. Map set of risk factors in the fishnet
```{r risk, echo=TRUE, message=FALSE, warning=FALSE}
# All variables in fishnet 
vars_net <- 
  rbind(abandon_cars, dead_animals,pothole,trash,graffiti,street_cleaning) %>%
  st_join(., cin_fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(cin_fishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  na.omit() %>% 
  dplyr::select(-`<NA>`) %>%
  ungroup()
### Multiple map for feature counts in fishnet
vars_net.long <- 
  gather(vars_net, Variable, value, -geometry, -uniqueID)
vars <- unique(vars_net.long$Variable)
mapList <- list()
for(i in vars){
  mapList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(vars_net.long, Variable == i), aes(fill=value), colour=NA) +
    scale_fill_viridis_c(option = "plasma",
                         name = " ") +
    labs(title=i) +
    mapTheme()}
do.call(grid.arrange,c(mapList, ncol =3, top = "Figure. 3.1 Risk Factors by Fishnet\n"))
## 3.2 Nearest Neighbor Feature
# convenience to reduce length of function names.
st_c <- st_coordinates
st_coid <- st_centroid
## create NN from abandoned cars, k = 3
'%!in%' <- function(x,y)!('%in%'(x,y))
vars_net$abandon_cars.nn <- 
  nn_function(st_c(st_coid(vars_net)),st_c(abandon_cars),k = 3)

vars_net$dead_animals.nn <-
           nn_function(st_c(st_coid(vars_net)),st_c(dead_animals),k = 3)

vars_net$pothole.nn <-
           nn_function(st_c(st_coid(vars_net)),st_c(pothole),k = 3)

vars_net$trash.nn <-
           nn_function(st_c(st_coid(vars_net)),st_c(trash),k = 3) 

vars_net$graffiti.nn <-
           nn_function(st_c(st_coid(vars_net)),st_c(graffiti),k = 3) 

vars_net$street_cleaning.nn <-
           nn_function(st_c(st_coid(vars_net)),st_c(street_cleaning),k = 3)
## Visualize the nearest three features
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
  gather(Variable, value, -geometry)
vars <- unique(vars_net.long.nn$Variable)
mapList <- list()
for(i in vars){
  mapList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
    scale_fill_viridis_c(option = "plasma",
                         name = " ") +
    labs(title=i) +
    mapTheme() +
    plotTheme()}
do.call(grid.arrange,c(mapList, ncol = 3, top = "Figure 3.2 Nearest Neighbor risk Factors by Fishnet\n"))

# IV and DVs all in fishnet
cin_final_net <-
  left_join(opioid_net, st_drop_geometry(vars_net), by="uniqueID") 
cin_final_net <-
  st_centroid(cin_final_net) %>%
  st_join(dplyr::select(cinNeigh, NEIGH), by = "uniqueID") %>%
  st_join(dplyr::select(cinPD, DISTRICT), by = "uniqueID") %>%
  st_drop_geometry() %>%
  left_join(dplyr::select(cin_final_net, geometry, uniqueID)) %>%
  st_sf() %>%
  na.omit()
```
### (III) Hotpots and Correlation Explorary Analysis
#### 4. Local Moran's I-related small multiple map of your outcome
As a critical part before build a robust model, it is imperative to figure out whether, where and how observed shooting cases cluster spatially. By leveraging the Local Moran’s I statistic, I locate areas in Philly where the clustering of shooting cases is greater than would be expected with random distribution. In Figure 4 we see that only some areas with high observed shooting cases have high clustering (high Moran’s I), such North Philly only some have a high level of confidence (low P Value) in their Moran’s I. Areas with high shooting case counts, clustering, and confidence are likely to be identified as Philly's shooting crime risk hotpot. 
With the hotspots identified, I continue engineer two new features for our model: shooting.isSig tells whether a cell is in a certain hotspot, and shooting.isSig.dist is the distance from a cell to its nearest hotspot. These two features allow us to factor in the spatial clustering of shooting.
```{r moran, echo=TRUE, message=FALSE, warning=FALSE}
## 4.1 spatial weights matrix
cin_final_net.nb <- poly2nb(as_Spatial(cin_final_net), queen=TRUE)
cin_final_net.weights <- nb2listw(cin_final_net.nb, style="W", zero.policy=TRUE)

local_morans <- localmoran(cin_final_net$countopioid,cin_final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

cin_final_net.localMorans <- 
  cbind(local_morans, as.data.frame(cin_final_net)) %>% 
  st_sf() %>%
  dplyr::select(opioid_Count = countopioid, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)

vars <- unique(cin_final_net.localMorans$Variable)
varList <- list()
for(i in vars){
  varList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(cin_final_net.localMorans, Variable == i), 
            aes(fill = Value), colour=NA) +
    scale_fill_viridis_c(option = "plasma",
                         name = " ") +
    labs(title=i,
         caption = "Figure 4. Local Moran's I Statistics, Observed Shooting Cases") +
    mapTheme() + theme(legend.position="bottom") +
    plotTheme()}
do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Burglary"))
### Distance to highly significant shooting hotpot
cin_final_net <- cin_final_net %>% 
  mutate(opioid.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(opioid.isSig.dist = 
           nn_function(st_c(st_coid(cin_final_net)),
                       st_c(st_coid(filter(cin_final_net, 
                                           opioid.isSig == 1))), 
                       k = 1))

ggplot() +
      geom_sf(data = cin_final_net, aes(fill=opioid.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Distance to highly significant weapon violation hot spot") +
      mapTheme()
```
#### 5. A small multiple scatterplot with correlations
Now with our complete set of risk factor features, we explore the correlation between the features and observed shooting crime Figure 5 below visualizes some of these correlations. It is intuitive that all the features selected are positively correlated to shooting crime case counts.
```{r scatterplot, echo=TRUE, message=FALSE, warning=FALSE}
# 5. A small multiple scatterplot with correlations
cin.correlation.long <-
  st_drop_geometry(cin_final_net) %>%
    dplyr::select(-uniqueID, -cvID, -NEIGH, -DISTRICT) %>%
    gather(Variable, Value, -countopioid)

cin.correlation.cor <-
  cin.correlation.long %>%
  group_by(Variable) %>%
  summarize(cin.correlation = cor(Value, countopioid, use = "complete.obs"))
ggplot(cin.correlation.long, aes(Value, countopioid)) +
  geom_point(size = 0.1) +
  geom_text(data = cin.correlation.cor, aes(label = paste("r =", round(cin.correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~Variable, ncol = 4, scales = "free") +
  labs(title = "Observed Shooting Crime as a Function of Risk Factors",
       subtitle = "Philadelphia, PA\n",
       caption = "Figure 5")+
  plotTheme()
```
#### 6. A histogram of dependent variable
Given shooting is a relatively rare event, it is reasonable for most grid cells to contain no crime events. And based on the histogram below, the distribution of the dependent variable is Poisson distribution, indicating a Poisson model should be considered for model building. 
```{r histogram, echo=TRUE, message=FALSE, warning=FALSE}
# 6. A histogram of dependent variable
ggplot(data = cin_final_net) +
  geom_histogram(aes(x = countopioid), fill = 'orange') +
  scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
  labs(title="Histogram of Dependent Variable: Opioid Overdose",
       subtitle = "Cincinnati, OH\n") +
  xlab('Count of Opioid') +
  ylab('Count') +
  plotTheme()
```
### (IV) Regression Model Building and Evaluation
#### 7. A small multiple map of model errors by random k-fold and spatial cross validation.
For a given risk factor, I selected feature counts in each fishnet and distance to shooting hotpots as well as distance to significant shooting hotpots as features used in the model to avoid collinerity. I also added Local Moran's I spatial process features based on the result in Question 4. Therefore, we have **eleven** features in total.
Goodness of fit metrics are generated for four regressions - two including *Just Risk Factors* (`reg.vars`), and a second (`reg.ss.vars`) includes risk factors plus the Local Moran's I Spatial Process features created
A well generalized crime predictive model learns the crime risk 'experience' at both citywide and local spatial scales. The best way to test for this is to hold out one local area, train the model on the remaining n - 1 areas, predict for the hold out, and record the goodness of fit, which is the cross-validation approach called LEAVE ONE GROUP OUT (LOGO-CV) used in the analysis. `reg.ss.cv` performs random k-fold cross validation using spatial process features, while `reg.ss.spatialCV` performs LOGO-CV, spatial cross-validation on neighborhood name, using the same features. Same processes are also conducted on *Just Risk Factors*. 
```{r fold, message=FALSE, warning=FALSE, include=FALSE}
## define the variables 
reg.vars <- c("abandon_cars.nn" , "dead_animals.nn","pothole.nn", "trash.nn","graffiti.nn","street_cleaning.nn")
reg.ss.vars <- c("abandon_cars.nn" , "dead_animals.nn","pothole.nn", "trash.nn","graffiti.nn", "street_cleaning.nn","opioid.isSig" ,"opioid.isSig.dist")
## Define crossValidate function
crossValidate <- function(dataset, id, dependentVariable, indVariables) {
  
  allPredictions <- data.frame()
  cvID_list <- unique(dataset[[id]])
  
  for (i in cvID_list) {
    
    thisFold <- i
    cat("This hold out fold is", thisFold, "\n")
    
    fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    
    regression <-
      glm(countopioid ~ ., family = "poisson", 
          data = fold.train %>% 
            dplyr::select(-geometry, -id))
    
    thisPrediction <- 
      mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
    allPredictions <-
      rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}
###  create random k-fold cv
reg.cv <- crossValidate(
  dataset = cin_final_net,
  id = "cvID",
  dependentVariable = "countopioid",
  indVariables = reg.vars) %>%
  dplyr::select(cvID = cvID, countopioid, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = cin_final_net,
  id = "cvID",
  dependentVariable = "countopioid",
  indVariables = reg.ss.vars) %>%
  dplyr::select(cvID = cvID, countopioid, Prediction, geometry)

###  create spatial cross validation
reg.spatialCV <- crossValidate(
  dataset = cin_final_net,
  id = "NEIGH",                            ### !!! <- really important line
  dependentVariable = "countopioid",
  indVariables = reg.vars) %>%
  dplyr::select(cvID = NEIGH, countopioid, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = cin_final_net,
  id = "NEIGH",                            ### !!! <- really important line
  dependentVariable = "countopioid",
  indVariables = reg.ss.vars) %>%
  dplyr::select(cvID = NEIGH, countopioid, Prediction, geometry)
# Bind four CVs together
reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction - countopioid,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - countopioid,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - countopioid,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - countopioid,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf() 
# calculate and visualize MAE for each fold 
error_by_reg_and_fold <- 
  reg.summary %>%
  group_by(Regression, cvID) %>% 
  summarize(Mean_Error = mean(Prediction - countopioid, na.rm = T),
            MAE = mean(abs(Mean_Error), na.rm = T),
            SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()
```
#### 7. Model errors by random k-fold and spatial cross validation
##### 7.1 Distribution of MAE
Figure 7.1 below displays the distribution of MAE across all models. Some small errors clustering can be captured, indicating the models perform well in terms of generalizability across the city.
```{r mae, echo=TRUE, message=FALSE, warning=FALSE}
#### 7.1 Distribution of MAE
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
  geom_histogram(bins = 30, colour="black", fill = "#5dccb9") +
  facet_wrap(~Regression) +  
  geom_vline(xintercept = 0) + 
  scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
  labs(title="Distribution of MAE", 
       subtitle = "k-fold cross validation vs. LOGO-CV\n",
       caption = "Figure 7.1",
       x="Mean Absolute Error", y="Count") +
  plotTheme()
```
##### 7.2 Visualizes the random k-fold and LOGO-CV errors spatially. 
Figure 7.2 provides additional information on the errors across models. 
```{r kfold, echo=TRUE, message=FALSE, warning=FALSE}
error_by_reg_and_fold %>%
  ggplot() +
  geom_sf(aes(fill = MAE)) +
  facet_wrap(~Regression) +
  scale_fill_viridis_c(option = "plasma") +
  labs(title = "Shooting Errors by Rondom k-fold and LOGO-CV Regression\n",
       caption = 'Figure 7.2') +
  mapTheme() + 
  theme(legend.position="bottom") +
  plotTheme()
```
##### 7.3 Predicted shooting and observed shooting
Interestingly, Figure 7.3 below shows that all models over-predict in low shooting-rate areas and under-predict in hot spot areas. Over-predictions in lower shooting areas may highlight areas of latent risk. Under-prediction in higher shooting areas may reflect difficulty predicting the hotspots.
```{r}
st_drop_geometry(reg.summary) %>%
  group_by(Regression) %>%
  mutate(opioid_Decile = ntile(countopioid, 10)) %>%
  group_by(Regression, opioid_Decile) %>%
  summarize(meanObserved = mean(countopioid, na.rm=T),
            meanPrediction = mean(Prediction, na.rm=T)) %>%
  gather(Variable, Value, -Regression, -opioid_Decile) %>%          
  ggplot(aes(opioid_Decile, Value, shape = Variable)) +
  geom_point(size = 2) + 
  geom_path(aes(group = opioid_Decile), colour = "black") +
  scale_shape_manual(values = c(2, 17)) +
  facet_wrap(~Regression) + xlim(0,10) +
  labs(title = "Predicted and Observed Shootings by Observed Shooting Decile\n",
       subtitle = "Cincinnati, OH\n",
       x = 'Opioid Decile') +
  plotTheme()
```
#### 8. A table of MAE and standard deviation MAE by regression.
Table 8 below confirms my assumption that the **Spatial Process** features improve the model.  The model appears consistently robust even in conservative LOGO-CV. For your reference, the average count of observed shooting cases in each cell is 0.0837, while the average count of predicted shooting cases in each cell is 0.0856.
```{r sdmae, echo=TRUE, message=FALSE, warning=FALSE}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
  summarize(Mean_MAE = round(mean(MAE), 2),
            SD_MAE = round(sd(MAE), 2)) %>%
  kable(caption = "Table 8. MAE by regression") %>%
  kable_styling("striped", full_width = F) 
```
#### 9. A table of raw errors by race context for a random k-fold vs. spatial cross validation regression.
To be generalizable, the models must produce errors on the similar level in both majority white and majority non-white neighborhoods. However, shown in Table 9, the MAE of `Majority_Non_White` neighborhoods is higher for non-white neighborhoods. In other word, each model on average, under-predicts in `Majority_Non_White` neighborhoods and over-predicts in `Majority_White` neighborhoods. It looks like this algorithm does not generalize well with respect to race.
```{r race, message=FALSE, warning=FALSE, include=FALSE}
## 9.1 Fetch census data
# View(load_variables(2018,'acs5',cache = TRUE))
tracts17.race <- 
  get_acs(geography = "tract", variables = c("B00001_001E","B02001_002E"), 
          year=2017, state="OH", county="Hamilton", geometry=T, output="wide") %>%
  st_transform('ESRI:102258') %>%
  rename(TotalPop = B00001_001E, 
         Whites = B02001_002E) %>%
  mutate(percentWhite = Whites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority_White", "Majority_Non_White")) %>%
  .[cinNeigh,]
```
```{r racemap, echo=TRUE, message=FALSE, warning=FALSE}
ggplot() + 
  geom_sf(data = na.omit(tracts17.race), aes(fill = raceContext)) +
  scale_fill_manual(values = c("#10cbaf", "#ff9966"), name="Race Context") +
  labs(title = "Race Context",
       subtitle = "Philadelphia, PA\n",
       caption = 'Figure 9') +
  mapTheme() + 
  theme(legend.position="bottom") +
  plotTheme()

reg.summary %>% 
  filter(str_detect(Regression, "LOGO")) %>%
    st_centroid() %>%
    st_join(tracts17.race) %>%
    na.omit() %>%
      st_drop_geometry() %>%
      group_by(Regression, raceContext) %>%
      summarize(mean.Error = mean(Error, na.rm = T)) %>%
      spread(raceContext, mean.Error) %>%
      kable(caption = "Table 9. Mean Error by Neighborhood Racial Context") %>%
        kable_styling("striped", full_width = F) 
  
```
### (IV) Model Alternative and Comparison
#### 10. The map comparing kernel density to risk predictions
In the last section, this analysis is aimed at figuring out if the predictive policing model is able to better predict shooting crime compared with traditional model - **Kernel Density Model**.
By making kernel density maps with different search radius scale (1000 Ft, 1500 Ft, and 2000 Ft), this analysis select the one with 1000 Ft search radius since it can better display the density of shooting cases (see Figure 10.1).
Next, a comparison map is generated of the risk categories for both model types with a sample of shooting cases in 2019 points overlaid. A strongly fit model should show that the highest risk category is uniquely targeted to places with a high density of shooting points. Indicated from Figure 10.2, although the traditional model can show the shooting risk pattern in 2019 more or less, the predictive policing model is far more accurate and direct.
```{r kerneldensity, echo=TRUE, message=FALSE, warning=FALSE}
### 10.1 Make Kernel Density Map
burg_ppp <- as.ppp(st_coordinates(opioid), W = st_bbox(cin_final_net))
burg_KD.1000 <- spatstat.core::density.ppp(burg_ppp, 1000)
burg_KD.1500 <- spatstat.core::density.ppp(burg_ppp, 1500)
burg_KD.2000 <- spatstat.core::density.ppp(burg_ppp, 2000)
burg_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1000), as(cinNeigh, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1500), as(cinNeigh, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.2000), as(cinNeigh, 'Spatial')))), Legend = "2000 Ft.")) 

burg_KD.df$Legend <- factor(burg_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=burg_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(cin_final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r shoot2019, message=FALSE, warning=FALSE, include=FALSE}
### 10.2 Fetch 2019 Shooting Victims Data
opioid17 <- opioid17%>%
  distinct() %>%
  .[cin_fishnet,] 
```

```{r kernel, echo=TRUE, message=FALSE, warning=FALSE}
burg_KDE_sf <- as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(cin_final_net)) %>%
  aggregate(., cin_final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category  <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(opioid17) %>% mutate(opioidCount = 1), ., sum) %>%
    mutate(opioidCount = replace_na(opioidCount, 0))) %>%
  dplyr::select(label, Risk_Category, opioidCount)
##### 10.4 Prediction by Risk Prediction Model
burg_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(opioid17) %>% mutate(opioidCount = 1), ., sum) %>%
      mutate(opioidCount = replace_na(opioidCount, 0))) %>%
  dplyr::select(label,Risk_Category, opioidCount)

rbind(burg_KDE_sf, burg_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(opioid17, 2500), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2016 opiod overdose risk predictions; 2018 opioid overdose") +
    mapTheme(title_size = 14)
```
#### 11. The bar plot making this comparison.
However, indicated from Figure 11, it is seemingly the advantages of predictive policing model is not consistent across all risk categories. For all risk categories lower than 90%, the traditional Kernel Density model is actually more accurate. For the highest category, however, the predictive policing risk model is more accurate. This finding might imply that the predictive policing model built in this analysis is better at locating shooting crime risk hotspots, and this again confirms the conclusion. 
```{r bar, echo=TRUE, message=FALSE, warning=FALSE}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countopioid = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_opioid = countopioid / sum(countopioid)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_opioid)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk prediction vs. Kernel density, 2017") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
## IV. Conclusion
Generally speaking, this model achieves the goal of identifying shooting crime risk hotspots in Philadelphia since it predicts hotspots more accurately than the traditional - Kernel Density model, and it identifies areas with no observed shooting crime at present that have latent shooting crime risk. Such findings and features are significant.
One of the limitations of this model is that it fails to avoid selection bias - it systematically under-predicts shooting crime in areas with high observed shooting crime cases and in majority non-white neighborhoods. This would have negative impact on the police force arrangement. 
Another limitation is clustered in the choice of dependent variable - shooting crime. In fact, there are subsets of shooting crimes which might raise further questions on how shooting cases are correlated with other risk factors and how the spatial pattern takes the role. For instance, as a subset of more general gun violence, [a drive-by shooting](https://popcenter.asu.edu/content/drive-shootings-0) refers to an incident when someone fires a gun from a vehicle at another vehicle, a person, a structure, or another stationary object that has been commonly neglected but has caused a comparatively high fatal rate. Hence, it is still quite early to draw the conclusion that either of the model is consistently better than the other since there are still many factors and scenarios left to be taken into consideration.  
Therefore, despite comparatively good performance on accuracy, any attempt to applying this model should be cautious about its limitations.
